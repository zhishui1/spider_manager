## nhsa_crawler.py 爬虫集成实施计划

### 1. 文件组织重构

**目标位置**: `backend/spiders/crawlers/nhsa_crawler.py`

**实施步骤**:
- 创建 `backend/spiders/crawlers/` 目录
- 将 `nhsa_crawler.py` 移动到该目录
- 重构为可导入的包结构，添加 `__init__.py` 和 `__main__.py`

### 2. Redis数据管理集成

**数据结构设计**:
```
Redis Key                    类型      用途
─────────────────────────────────────────────────
spider:nhsa:urls            Set       URL去重集合
spider:nhsa:queue           List      待爬取URL队列
spider:nhsa:status          String    爬虫运行状态
spider:nhsa:progress        Hash      爬取进度信息
spider:nhsa:error_count     String    错误计数
```

**实施步骤**:
- 创建 `backend/spiders/redis_manager.py` 统一管理Redis操作
- 在爬虫脚本中添加Redis去重逻辑
- 实现队列管理功能

### 3. 爬虫控制逻辑增强

**新增功能**:
- 信号处理：支持 SIGTERM、SIGINT 信号实现优雅停止
- 状态报告：定期向Redis写入状态信息
- 进度追踪：记录已爬取数量、当前类别、错误数量

**关键代码变更**:
```python
# 添加信号处理
import signal
import sys

def signal_handler(signum, frame):
    cleanup()
    sys.exit(0)

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

# 状态上报
def update_status(status: str, progress: dict):
    r.set(f'spider:nhsa:status', status)
    r.hset(f'spider:nhsa:progress', mapping=progress)
```

### 4. 适配器增强 (adapters.py)

**优化现有 NHSASpiderAdapter**:
- 使用Redis获取实时状态替代子进程轮询
- 实现暂停/恢复功能（向Redis写入控制标志）
- 增强统计数据获取逻辑

**新增方法**:
```python
def get_detailed_status(self) -> Dict[str, Any]:
    """获取详细状态包括进度信息"""

def is_paused(self) -> bool:
    """检查是否暂停"""

def cleanup(self):
    """资源清理"""
```

### 5. 状态同步机制

**实时同步方案**:
- 后端定期从Redis读取状态
- 通过WebSocket推送到前端
- 前端store订阅状态更新

**实施步骤**:
- 在 `consumers.py` 中添加状态订阅逻辑
- 在 `backend/spiders/state_manager.py` 中实现状态管理

### 6. 数据统计功能完善

**统计指标**:
- `total_items`: JSON文件记录条数
- `file_count`: 下载文件数量
- `categories`: 按类别分组统计
- `date_range`: 数据时间范围
- `success_rate`: 爬取成功率

### 7. 错误处理机制

**实现方案**:
- 全局异常捕获装饰器
- 错误日志记录到Redis和文件
- 错误状态实时推送到前端
- 失败重试机制（最多3次）

### 8. 测试验证

**测试用例**:
- 爬虫启动/停止/暂停/恢复
- Redis去重功能
- 数据统计准确性
- 错误处理和恢复
- 前端状态同步